#!/bin/bash

set -x

# Acquire an exclusive lock
exec {LOCK_FD}>/var/lock/cs50-docker-cmd.lock
flock --exclusive --nonblock $LOCK_FD || exit 1

# Ignore . and .. folders
shopt -s extglob
export GLOBIGNORE=1

if [ -z "$CS50_IDE_S3_DESTINATION" ]; then
    echo "s3 destination is empty or not set"
    exit 1
fi

# TODO parameterize
DEFAULT_RETRY_COUNT=3
UPLOAD_INTERVAL=1800
USER_FOLDER_PATH="/home/ubuntu"


function init() {
    source /etc/cs50/secrets/aws-credentials
    S3CMD="s3cmd --access_key=$AWS_ACCESS_KEY_ID \
        --access_token=$AWS_SESSION_TOKEN \
        --secret_key=$AWS_SECRET_ACCESS_KEY \
        --region=us-west-2"

    TAR_GZ_DEST="$(mktemp --suffix=.tar.gz)"
}


function download() {
    init
    $S3CMD get --force "$CS50_IDE_S3_DESTINATION" "$TAR_GZ_DEST" && \

        # Using sudo to apply original file ownerships and permissions
        sudo tar --directory="$USER_FOLDER_PATH" --extract --gzip --verbose --file="$TAR_GZ_DEST"

    # status=$?

    # # Not found
    # if [ $status -eq 12 ]; then
    #     status=0
    # fi

    cleanup
    # return $status
}


function upload() {

    exec {UPLOAD_LOCK_FD}>/var/lock/cs50-docker-cmd-upload.lock
    flock --wait=60 $UPLOAD_LOCK_FD || exit 1
    init
    pushd $USER_FOLDER_PATH
    tar --create --gzip --verbose --ignore-failed-read --file="$TAR_GZ_DEST" !(.c9) .c9/metadata .c9/cs50 .c9/*/collab* .c9/*.settings && \
        $S3CMD put "$TAR_GZ_DEST" "$CS50_IDE_S3_DESTINATION"

    status=$?
    popd
    cleanup
    flock --unlock $UPLOAD_LOCK_FD
    return $status
}


function cleanup() {
    rm --force "$TAR_GZ_DEST"
}


function teardown() {
    # Clear the trap
    trap - INT TERM EXIT
    retry_count=$DEFAULT_RETRY_COUNT
    until upload || [ $retry_count -eq 0 ]; do
        ((retry_count -= 1))
        sleep 5
    done

    if [ $retry_count -eq 0 ]; then
        echo "failed to upload user data"
        exit 1
    fi

    exit 0
}


retry_count=$DEFAULT_RETRY_COUNT
until download || [ $retry_count -eq 0 ]; do
    ((retry_count -= 1))
    sleep 5
done

if [ $retry_count -eq 0 ]; then
    echo "failed to download user data"
    exit 1
fi

trap "teardown" INT TERM EXIT

# Start ssh daemon
sudo /usr/sbin/sshd -E /var/log/sshd.log

retry_count=$DEFAULT_RETRY_COUNT
sleep $UPLOAD_INTERVAL &

# Wait on sleep to be able to interrupt with signal
wait $!
until [ $retry_count -eq 0 ]; do
    upload
    if [ $? -eq 0 ]; then
        retry_count=$DEFAULT_RETRY_COUNT
        sleep $UPLOAD_INTERVAL &
        wait $!
    else
        (( retry_count -= 1 ))
        sleep 5
    fi
done

if [ $retry_count -eq 0 ]; then
    echo "failed to upload user data"
    exit 1
fi
